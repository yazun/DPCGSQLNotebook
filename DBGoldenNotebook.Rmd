---
title: "DPCG SQL Golden Database Notebook."
author: "Krzysztof Nienartowicz"
output:
  html_document:
    fig_height: 10
    fig_width: 18
    df_print: paged
    code_folding: hide
    toc: yes
    toc_float: yes
    number_sections: yes
    theme: spacelab
    highlight: pygments
    as.iframe: yes
    code_download: true
  html_notebook:
    code_download: true
    as.iframe: yes
    code_folding: show
    fig_height: 10
    fig_width: 18
    highlight: pygments
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
params:
  inRunId: 3852
  hostname: gaiadb12i
  dbPort: 55431
  dbUser: nienarto_local
  tmpSchema: dr3_ops_cs36_tmp
  templateName: Default
---

```{=html}
<!--

Explanation of the above params section.
These parameters are set and can be overriden via the command line if knitting via VariDashboard or terminal.
Except runName, all parameters are required.

Change dbUser to your DB user when running editing this file in RStudio.

-->
```
```{=html}
<style>
    body .main-container {
        max-width: 80%;
        margin-left: auto;
        margin-right: auto;
}

h1, .h1, h2, .h2, h3, .h3 {
    margin-top: 74px;
}

</style>
```
```{r setup, include=FALSE, echo=FALSE, cache=FALSE, message=FALSE}
# load helper DPCG package
require(dpcgR)

require(dplyr)
require(glue)
require(bit64)
require(tibble)
require(plotly)
require(viridis)
require(RColorBrewer)

options(width = 1600)
options(warn = 0, cache=FALSE, autodep=TRUE, scipen = 999)

# Connect to DB then set the default connection for r-notebook chunks so we do not have to provide it every time with {sql connection = conn}
conn<-dpcgR::connect(hostname=params$hostname,port=params$dbPort, user = params$dbUser)

all_times <- list() 
knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options, envir) {
    if (before) {
      # record the current time before each chunk
      now <<- Sys.time()
    } else {
      # calculate the time difference after a chunk
      res <- difftime(Sys.time(), now)
      # return a character string to show the time
      # paste("Time for this code chunk to run:", res)
      all_times[[options$label]] <<- res
      
    }
  }
}))
# connection to be used by the sql chunks, default size of the plots
knitr::opts_chunk$set(connection = "conn", fig.height=10, fig.width=12, 
                      cache=FALSE,
                      autodep=TRUE,  
                      error = TRUE, # do not interrupt in case of errors
                      dev = "ragg_png",
                      class.source = 'fold-show',
                      time_it = TRUE
                      # attr.source='.numberLines'
                      
                      )

```
___
<center>
![](./Resources/dpac_logo.png){width=50%}


___

<font size=20 color="BLUE">`r rmarkdown::metadata$title` </font>

___

</center>

<font size=5>

|              |              |
|------------- | -------------|
|Prepared by:  | `r rmarkdown::metadata$author`|
|Approved by:  | Laurent Eyer |
|reference:    | GAIA-DG-TN-GEN-KN-013|
|issue:        | 01|
|revision:     | 1|
|date:         | `r Sys.Date()`|
|status:       | Draft |
</font>

---

# Document History

<font size=4>

|Issue  | Revision | Date        | Author   | Comment         |
|------ | ---------| ----------- |--------- |-----------------|
|D      |       1  | 2022-01-10  |KN        | Initial version |
|D      |       2  | 2022-01-31  |KN        | PLJava ts operators added |

</font>

---

# Introduction.

This is collection of SQL queries that DPCG endorses and will maintain their usability as long as possible. 
You can copy-paste and adapt these queries for your needs. 
As usual, in case of doubt, you can ask at [DPCG database channel](https://gaiamattermost.isdc.unige.ch/dpcg/channels/dpcg-database).

This notebook with all queries below are also part of Continuous Integration efforts that DPCG will treat as any other software project, intercepting breaking changes or performance regressions.

Queries below are self-contained and must work with DPCG Gaia database based on Postgres-XL/XZ/TBase.
They show all aspects of the system usage, from input to results, validation, visualisation.


# Setup, Prerequisisties

You should have your `$HOME/.pgpass` file set up with non-world readable mask (0600). Similarly as for psql and other PostgreSQL clients.
dpcgR R package is a wrpper around Postgres R client and gives number of R primitives for DB interaction and visualisations based on the VariOM definitions.

## Useful SQL options to set

There are few useful options to set before starting each interactive SQL session.
We advise to call the code below in the .psqlrc or everytime you start a session (also in a client code) as defulat settings are more suitable for massive processing perfromaed with the VariFramwork.

```{sql sql_setup}
-- set search path to include _mv schema for convenience
set search_path to dr3_ops_cs36, dr3_ops_cs36_mv, public;
```
```{sql setup_for_user_queries}
-- enable some options that iusually speed up analytical queries (i.e. having joins)
select setDPCGAnalytics();

```

# Derived Timeseries 

In order to get derived timeseries for a selection of sourceids, runid and operator name we can:

```{sql sql_ts_der_1}
select (dr3_ops_cs36.op_test1(sourceid,'RemoveOutliersFaintAndBrightOperator_FOV_G')).* from 
(select  sourceid from dr3_ops_cs36_mv.v_final_dr3_export_helper  limit 5)  ;
```

The function takes run configuration, caches it for the session and executes VariFrameowrk pipeline in plJava VM.


# Sampling 
## Sample from a single table

Using **_tablesample_** SQL keyword we scan pages of the tables obtaining quasi-random sampling. 
Keep in mind **_tablesample_** is always applied **before** the where or join clauses.

From our source table
```{sql sample_1}
-- calculate required percentage based on the input set size.
-- select round((10^6*100)/(size),4) perc from catalog where fname = 'GAIA_DR3_ALL'
-- 0.0392 for 1M sample

select 
  sourceid, r_fluxtoMagVal(gfluxmean,getzeropoint('Gaia','GAIA_PHOT_G')) -- convert to mag using R function
from source tablesample bernoulli(0.0392)
limit 10^6

```
	
takes 3 secs, but is quasi-random.

Calculated G Mag on the fly using plR function:

```{sql sample, output.var="result"}
select sourceid, r_fluxtoMagVal(gfluxmean,getzeropoint('Gaia','GAIA_PHOT_G')) g_mag -- convert to mag
from source tablesample system(0.0392)
limit 10^2
```

This is much slower, but might statistically more sound, related to the sampling method (Bernoulli) :

```{sql sample_2, output.var="result"}
 select sourceid, r_fluxtoMagVal(gfluxmean,getzeropoint('Gaia','GAIA_PHOT_G'))  g_mag -- convert to mag
from source tablesample bernoulli(0.0392)
limit 10^2
```

Both methods are very IO intensive.

# Derived catalogs with results

## With supervised classification results.
```{sql derived_supervised_classificaiton_results9}
select 
  sourceid, 
  (avals(bestclassificationestimates))[1]::float8 as prob, 
  getAttributesStrForPattern(getcatalogname(dcatalogid), 'primaryVarType', externalattributese) as catalogtype 
from 
  supervisedclassificationresult s join v_source_catalog_derived using (catalogid,sourceid) 
where 
runid = 90002 
and catalogid=getmaincatalog() 
and dcatalogid = getcatalogid('DR3_ALL_XM_MERGED_ALL_V8.3') 
and classifierid=getclassifierid('CD_NOV20_META1') 
limit 10

```

# SOS queries

# List of SOS views with fields from respective packages

We have specific SOS tables that each package might use at your convenience:

```{sql list_sos_tables}
select  table_name "SOS table name" from information_schema.tables where table_name ~ '^v_sos_' order by 1;

```
Which should be used as usual with runid (the `catalogid` is not needed):
For example to check AGN attributes for a specific run:

```{sql sos_agn_1}
select * from v_sos_agnattributes 
  where runid = 3852 
  limit 5
```

# Spectra handling

In order to fetch BP/RP spectra for a single source execute:

```{sql spectra_single_source}
select
sourceid,
alpha, alphastarerror, delta, deltaerror, varpi, varpierror, mualphastar, mualphastarerror, mudelta, mudeltaerror, radialvelocity, radialvelocityerror,
  r_pseudo_wavelength(
 (bprp).bprefalpos
  ,500,30) as pseudo_wavelength
 ,(bprp).bpspectralshape
 ,(bprp).bpobstime
from dr3_ops_cs36.tsspectra
join source using (sourceid)
join lateral (select  getspectra(spec) bprp) bprp  on true
where
 catalogid = getmaincatalog()
and sourceid = 2926825431170093184

```
The heart of this query is `getspectra` function which translates from compressed spectra form to Gaia transitid rows.
Currently we do not support multidimensional sets of spectra returned: the function returns unnested spectra already, that might need ordering if more sources are involved.

# Analytical materialized tables


At each cycle we create number of tables that are flat representations of results, usually including basci information about the Sources, with source results (i.e. GVD), timeseries results, classification results for convenient check of the results.
Such queries are made only for major runs if no requested by users.

Information about such materialized tables that were created by the _parallel script_ is kept in the `mv_table_metainfo` and one can query

## Main materialized tables


## Environment calculation 


# Timing summary

```{r}
print(all_times)
```


# Appendix: DB functions available

# Appendix: Parallel script